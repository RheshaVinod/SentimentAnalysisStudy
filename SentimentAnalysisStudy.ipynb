{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IIhHVbOBEuJK",
        "outputId": "8d81a83d-c9db-4830-b48b-2ac38c9f1014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.25.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install torch torchvision\n",
        "!pip install scikit-learn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q4Oijo3zE47s",
        "outputId": "35148cd6-db2f-4d71-bf6d-b9a187a76d31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eiSN5mDXFBi5",
        "outputId": "1fe65606-9b16-4432-a32f-0703a480e4a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bf776fc6490>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Dataset Generation\n",
        "\n",
        "- Load the HW1 Amazon CSV (must have columns `reviewText` and `overall`).\n",
        "- Build a balanced dataset: *PER_RATING_COUNT* samples for each rating 1..5.\n",
        "- Map ratings to ternary labels: >3 → 1 (pos), <3 → 2 (neg), ==3 → 3 (neutral).\n",
        "- Save to cache to avoid re-computation."
      ],
      "metadata": {
        "id": "QZJKPYQg-L1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    file_path,\n",
        "    sep='\\t',\n",
        "    engine='python',       # FIXES your parser error\n",
        "    on_bad_lines='skip',   # skips malformed rows\n",
        "\n",
        ")\n",
        "\n",
        "print(\"Loaded shape:\", df.shape)\n",
        "print(df.columns)\n",
        "df = df[['star_rating', 'review_body']]\n",
        "\n",
        "df['star_rating'] = df['star_rating'].astype(str)\n",
        "\n",
        "print(df.head())\n",
        "print(df['star_rating'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PEEoGg-kFKDh",
        "outputId": "1fcbf486-e4e3-4c9b-ba66-932216ff65f9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded shape: (2637159, 15)\n",
            "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
            "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
            "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
            "       'review_headline', 'review_body', 'review_date'],\n",
            "      dtype='object')\n",
            "  star_rating                                        review_body\n",
            "0           5                                     Great product.\n",
            "1           5  What's to say about this commodity item except...\n",
            "2           5    Haven't used yet, but I am sure I will like it.\n",
            "3           1  Although this was labeled as &#34;new&#34; the...\n",
            "4           4                    Gorgeous colors and easy to use\n",
            "star_rating\n",
            "5    1580941\n",
            "4     417975\n",
            "1     306576\n",
            "3     193452\n",
            "2     138215\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples_per_rating = 50000\n",
        "random_state = 42\n",
        "\n",
        "balanced_dfs = []\n",
        "\n",
        "for rating in ['1', '2', '3', '4', '5']:\n",
        "    subset = df[df['star_rating'] == rating]\n",
        "\n",
        "    sampled_subset = subset.sample(\n",
        "        n=samples_per_rating,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    balanced_dfs.append(sampled_subset)\n",
        "\n",
        "balanced_df = pd.concat(balanced_dfs)\n",
        "\n",
        "# Shuffle final dataset\n",
        "balanced_df = balanced_df.sample(\n",
        "    frac=1,\n",
        "    random_state=random_state\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"Balanced shape:\", balanced_df.shape)\n",
        "print(balanced_df['star_rating'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uOPjnFBg3W64",
        "outputId": "91f45257-9cd7-4693-b11d-084cc216eb12"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced shape: (250000, 2)\n",
            "star_rating\n",
            "1    50000\n",
            "2    50000\n",
            "3    50000\n",
            "4    50000\n",
            "5    50000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.to_csv(\"balanced_250k.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Ebwmftgq3aKb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# CONFIG\n",
        "BALANCED_CSV = \"balanced_250k.csv\"\n",
        "OUTPUT_DIR = \"processed_data\"\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "balanced_df = pd.read_csv(BALANCED_CSV)\n",
        "\n",
        "def rating_to_sentiment(r):\n",
        "    r = float(r)\n",
        "    if r > 3:\n",
        "        return 1  # positive\n",
        "    elif r < 3:\n",
        "        return 2  # negative\n",
        "    else:\n",
        "        return 3  # neutral\n",
        "\n",
        "balanced_df['sentiment'] = balanced_df['star_rating'].apply(rating_to_sentiment)\n",
        "\n",
        "print(\"Star rating distribution:\")\n",
        "print(balanced_df['star_rating'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nSentiment distribution:\")\n",
        "print(balanced_df['sentiment'].value_counts().sort_index())\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    balanced_df,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=balanced_df['sentiment']\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain size: {len(train_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")\n",
        "\n",
        "\n",
        "train_df.to_csv(os.path.join(OUTPUT_DIR, \"train_80pct.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(OUTPUT_DIR, \"test_20pct.csv\"), index=False)\n",
        "\n",
        "\n",
        "torch.save({\n",
        "    'train_reviews': train_df['review_body'].astype(str).tolist(),\n",
        "    'train_sentiments': torch.tensor(train_df['sentiment'].values, dtype=torch.long),\n",
        "    'test_reviews': test_df['review_body'].astype(str).tolist(),\n",
        "    'test_sentiments': torch.tensor(test_df['sentiment'].values, dtype=torch.long),\n",
        "}, os.path.join(OUTPUT_DIR, \"balanced_250k_torch.pt\"))\n",
        "\n",
        "print(\"Saved everything successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "m_f3wKUhFz3d",
        "outputId": "4cddd82b-4f60-4d0a-ba08-db9ef597c621"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Star rating distribution:\n",
            "star_rating\n",
            "1    50000\n",
            "2    50000\n",
            "3    50000\n",
            "4    50000\n",
            "5    50000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "1    100000\n",
            "2    100000\n",
            "3     50000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train size: 200000\n",
            "Test size: 50000\n",
            "Saved everything successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Word Embedding\n",
        "-Load the pretrained “word2vec-google-news-300”\n",
        "-Train a Word2Vec model using your own dataset\n"
      ],
      "metadata": {
        "id": "dSJIO0tNGxYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_pretrained = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "print(\"Loaded successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hr2NG41uGsxB",
        "outputId": "270cf325-9d81-4346-9a72-15be49f0ca42"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_pretrained.most_similar(\n",
        "    positive=['king','woman'],\n",
        "    negative=['man']\n",
        ")\n"
      ],
      "metadata": {
        "id": "StHQzAKEG0Dj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ef9a4717-7549-4c06-81ef-cb9b43bfea15"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951),\n",
              " ('crown_prince', 0.5499460697174072),\n",
              " ('prince', 0.5377321839332581),\n",
              " ('kings', 0.5236844420433044),\n",
              " ('Queen_Consort', 0.5235945582389832),\n",
              " ('queens', 0.518113374710083),\n",
              " ('sultan', 0.5098593235015869),\n",
              " ('monarchy', 0.5087411403656006)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_pretrained.similarity('excellent','outstanding')"
      ],
      "metadata": {
        "id": "28mjf-GyG1rx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "df9ae5fb-0e90-4e9d-f018-0c7122dfc286"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.55674857)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewSentenceIterator:\n",
        "    def __init__(self, csv_path, text_col='review_body'):\n",
        "        self.csv_path = csv_path\n",
        "        self.text_col = text_col\n",
        "\n",
        "    def __iter__(self):\n",
        "        for chunk in pd.read_csv(self.csv_path, usecols=[self.text_col], chunksize=5000, dtype=str):\n",
        "            for text in chunk[self.text_col].astype(str).values:\n",
        "                yield simple_preprocess(text, deacc=True)\n"
      ],
      "metadata": {
        "id": "vtc7OGBeG3r8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "TRAIN_CSV = \"processed_data/train_80pct.csv\"\n",
        "MODEL_OUT_DIR = \"models\"\n",
        "os.makedirs(MODEL_OUT_DIR, exist_ok=True)\n",
        "MODEL_PATH = os.path.join(MODEL_OUT_DIR, \"w2v_amazon_office_products_300_window11_min10_sg1.bin\")\n",
        "\n",
        "VECTOR_SIZE = 300\n",
        "WINDOW = 11\n",
        "MIN_COUNT = 10\n",
        "SEED = 42\n",
        "WORKERS = 4\n",
        "SG = 1\n",
        "\n",
        "sentences = ReviewSentenceIterator(TRAIN_CSV, text_col='review_body')\n",
        "\n",
        "print(\"Building vocabulary...\")\n",
        "model = Word2Vec(\n",
        "    vector_size=VECTOR_SIZE,\n",
        "    window=WINDOW,\n",
        "    min_count=MIN_COUNT,\n",
        "    workers=WORKERS,\n",
        "    seed=SEED,\n",
        "    sg=SG\n",
        ")\n",
        "\n",
        "model.build_vocab(sentences)\n",
        "print(f\"Vocab size after build_vocab: {len(model.wv.key_to_index)}\")\n",
        "\n",
        "print(\"Training model (this can take a while)...\")\n",
        "model.train(\n",
        "    ReviewSentenceIterator(TRAIN_CSV, text_col='review_body'),\n",
        "    total_examples=model.corpus_count,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "model.save(MODEL_PATH)\n",
        "model.wv.save_word2vec_format(MODEL_PATH + \".kv.bin\", binary=True)\n",
        "print(\"Saved model to:\", MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1pwoK7tbUDsP",
        "outputId": "5114f642-44bf-4c4a-97b2-3cd154a3df00"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocabulary...\n",
            "Vocab size after build_vocab: 15017\n",
            "Training model (this can take a while)...\n",
            "Saved model to: models/w2v_amazon_office_products_300_window11_min10_sg1.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "w2v_own = Word2Vec.load(MODEL_PATH)\n",
        "\n",
        "# analogy: king - man + woman\n",
        "try:\n",
        "    results = w2v_own.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=10)\n",
        "    print(\"Analogy (own model) king - man + woman -> top results:\")\n",
        "    for w, score in results[:6]:\n",
        "        print(f\"  {w:15s} {score:.4f}\")\n",
        "except KeyError as e:\n",
        "    print(\"Analogy failed (word not in vocab):\", e)\n",
        "\n",
        "# similarity: excellent vs outstanding\n",
        "try:\n",
        "    sim_own = w2v_own.wv.similarity('excellent', 'outstanding')\n",
        "    print(f\"\\nSimilarity(own model) excellent vs outstanding = {sim_own:.6f}\")\n",
        "except KeyError as e:\n",
        "    print(\"Similarity failed (word not in vocab):\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JaG3AuSfNtsy",
        "outputId": "40d646ac-bc49-4811-a786-8101d6e166aa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy (own model) king - man + woman -> top results:\n",
            "  magnum          0.5426\n",
            "  chisel          0.3962\n",
            "  gamers          0.3940\n",
            "  urban           0.3924\n",
            "  bros            0.3888\n",
            "  rb              0.3885\n",
            "\n",
            "Similarity(own model) excellent vs outstanding = 0.722054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Simple models"
      ],
      "metadata": {
        "id": "p4yJmzB4IC3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(\"processed_data/train_80pct.csv\")\n",
        "test_df = pd.read_csv(\"processed_data/test_20pct.csv\")\n",
        "\n",
        "# Keep only class 1 and 2 (binary)\n",
        "train_df = train_df[train_df['sentiment'].isin([1,2])]\n",
        "test_df = test_df[test_df['sentiment'].isin([1,2])]\n",
        "\n",
        "print(\"Train shape (binary):\", train_df.shape)\n",
        "print(\"Test shape (binary):\", test_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RTxQmCM982yz",
        "outputId": "700cf4dd-70d6-43b5-ff3e-17f1c922a121"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape (binary): (160000, 3)\n",
            "Test shape (binary): (40000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# downloads (run once; safe to call repeatedly)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "# setup\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# text cleaning\n",
        "def clean_review(text):\n",
        "    text = str(text).lower()                       # lowercase and ensure string\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)             # remove HTML tags\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)    # remove URLs\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)          # keep letters and spaces only\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()       # collapse & trim whitespace\n",
        "    return text\n",
        "\n",
        "# lemmatize plain whitespace-tokenized string\n",
        "def lemmatize_review(text):\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# remove stopwords from plain whitespace-tokenized string\n",
        "def remove_stopwords(text):\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# unified preprocess: clean -> lemmatize -> remove stopwords -> gensim tokenize\n",
        "def preprocess(text):\n",
        "    text = clean_review(text)\n",
        "    text = lemmatize_review(text)\n",
        "    text = remove_stopwords(text)\n",
        "    # final tokenization with gensim's simple_preprocess (deacc removes accents/punctuation)\n",
        "    return simple_preprocess(text, deacc=True)\n",
        "\n",
        "# safe vector lookup & averaging (works if 'model' is Word2Vec or KeyedVectors)\n",
        "def review_to_avg_vector(review, model):\n",
        "    # get kv = model.wv if necessary (works both when model is Word2Vec or KeyedVectors)\n",
        "    kv = getattr(model, \"wv\", model)\n",
        "\n",
        "    tokens = preprocess(review)\n",
        "    vectors = []\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in kv.key_to_index:\n",
        "            vectors.append(kv[word])\n",
        "\n",
        "    # return zero vector if none in vocab\n",
        "    if len(vectors) == 0:\n",
        "        # try to determine vector_size safely\n",
        "        vector_size = getattr(kv, \"vector_size\", None)\n",
        "        if vector_size is None:\n",
        "            # fallback for older gensim APIs\n",
        "            vector_size = getattr(model, \"vector_size\", 300)\n",
        "        return np.zeros(vector_size, dtype=float)\n",
        "\n",
        "    return np.mean(vectors, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5HIoJucf85Iy",
        "outputId": "ead39905-93ae-4e07-b466-f3314b220435"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train and X_test using pretrained model\n",
        "X_train_pre = np.array([\n",
        "    review_to_avg_vector(text, w2v_pretrained)\n",
        "    for text in train_df['review_body']\n",
        "])\n",
        "\n",
        "X_test_pre = np.array([\n",
        "    review_to_avg_vector(text, w2v_pretrained)\n",
        "    for text in test_df['review_body']\n",
        "])\n",
        "\n",
        "y_train = train_df['sentiment'].values\n",
        "y_test = test_df['sentiment'].values\n"
      ],
      "metadata": {
        "id": "CjQEWcTS86zk"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_own = np.array([\n",
        "    review_to_avg_vector(text, w2v_own.wv)\n",
        "    for text in train_df['review_body']\n",
        "])\n",
        "\n",
        "X_test_own = np.array([\n",
        "    review_to_avg_vector(text, w2v_own.wv)\n",
        "    for text in test_df['review_body']\n",
        "])\n"
      ],
      "metadata": {
        "id": "GCHfUtkL88vz"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron\n",
        "perc_pre = Perceptron(random_state=42)\n",
        "perc_pre.fit(X_train_pre, y_train)\n",
        "y_pred = perc_pre.predict(X_test_pre)\n",
        "acc_perc_pre = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# SVM\n",
        "svm_pre = LinearSVC(random_state=42)\n",
        "svm_pre.fit(X_train_pre, y_train)\n",
        "y_pred = svm_pre.predict(X_test_pre)\n",
        "acc_svm_pre = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Pretrained - Perceptron Accuracy:\", acc_perc_pre)\n",
        "print(\"Pretrained - SVM Accuracy:\", acc_svm_pre)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5ypAbgUe8-zD",
        "outputId": "9605efd2-270a-4675-8ca8-bc34573e616f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained - Perceptron Accuracy: 0.7858\n",
            "Pretrained - SVM Accuracy: 0.817275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron\n",
        "perc_own = Perceptron(random_state=42)\n",
        "perc_own.fit(X_train_own, y_train)\n",
        "y_pred = perc_own.predict(X_test_own)\n",
        "acc_perc_own = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# SVM\n",
        "svm_own = LinearSVC(random_state=42)\n",
        "svm_own.fit(X_train_own, y_train)\n",
        "y_pred = svm_own.predict(X_test_own)\n",
        "acc_svm_own = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Own W2V - Perceptron Accuracy:\", acc_perc_own)\n",
        "print(\"Own W2V - SVM Accuracy:\", acc_svm_own)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o0CJdtkR9AjA",
        "outputId": "2ad4a24a-fa5c-4815-c9bc-de2a0f0325e6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Own W2V - Perceptron Accuracy: 0.731125\n",
            "Own W2V - SVM Accuracy: 0.855025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Feedforward Neural Networks"
      ],
      "metadata": {
        "id": "LgqtUKl7TJp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TRAIN_CSV = \"processed_data/train_80pct.csv\"\n",
        "TEST_CSV  = \"processed_data/test_20pct.csv\"\n",
        "W2V_PRETRAINED_PATH = None   # if you have local preloaded key-vector file (optional)\n",
        "W2V_OWN_PATH = \"models/w2v_amazon_office_products_300_window11_min10_sg1.bin\"  # as per your earlier code\n",
        "\n",
        "# Vector size used when training your own model (must match)\n",
        "VECTOR_SIZE = 300\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
        "print(\"Sentiment unique values:\", sorted(train_df['sentiment'].unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Av8ppf5kOFAY",
        "outputId": "0d42e4da-ebc4-4012-e5fa-463ff72c672d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (200000, 3) Test shape: (50000, 3)\n",
            "Sentiment unique values: [np.int64(1), np.int64(2), np.int64(3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "\n",
        "USE_PRETRAINED = True    # set False to skip Google-News pretrained (big download)\n",
        "USE_OWN = True           # set True to load your own trained model file\n",
        "\n",
        "w2v_pretrained = None\n",
        "w2v_own = None\n",
        "\n",
        "if USE_PRETRAINED:\n",
        "    try:\n",
        "        # if you previously downloaded 'word2vec-google-news-300' via gensim api, you can use it here\n",
        "        import gensim.downloader as api\n",
        "        print(\"Loading pretrained GoogleNews (this is large; skip if not available)...\")\n",
        "        w2v_pretrained = api.load(\"word2vec-google-news-300\")\n",
        "        print(\"Loaded GoogleNews model.\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not load pretrained via gensim API (skip or load manually).\", e)\n",
        "        w2v_pretrained = None\n",
        "\n",
        "if USE_OWN:\n",
        "    try:\n",
        "        print(\"Loading your own Word2Vec model from:\", W2V_OWN_PATH)\n",
        "        w2v_own = Word2Vec.load(W2V_OWN_PATH)\n",
        "        print(\"Loaded own model; vocab size:\", len(w2v_own.wv.key_to_index))\n",
        "    except Exception as e:\n",
        "        print(\"Couldn't load own model:\", e)\n",
        "        w2v_own = None\n",
        "\n",
        "# Choose the embedding to use later by name: 'pretrained' | 'own'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qCVzmhKRU2o2",
        "outputId": "35805e62-4c17-4dc0-a212-431dcac60294"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained GoogleNews (this is large; skip if not available)...\n",
            "Loaded GoogleNews model.\n",
            "Loading your own Word2Vec model from: models/w2v_amazon_office_products_300_window11_min10_sg1.bin\n",
            "Loaded own model; vocab size: 15017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    text = clean_text(text)\n",
        "    return simple_preprocess(text, deacc=True)  # list of tokens"
      ],
      "metadata": {
        "id": "acqg2KE9_1F5"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def avg_w2v_vector(tokens, kv, vector_size=VECTOR_SIZE):\n",
        "    \"\"\"\n",
        "    Returns average vector (numpy array). If no tokens in vocab -> zero vector.\n",
        "    kv may be a KeyedVectors or Word2Vec.wv object.\n",
        "    \"\"\"\n",
        "    if kv is None:\n",
        "        return np.zeros(vector_size, dtype=np.float32)\n",
        "    vecs = []\n",
        "    for t in tokens:\n",
        "        if t in kv.key_to_index:\n",
        "            vecs.append(kv[t])\n",
        "    if len(vecs) == 0:\n",
        "        return np.zeros(vector_size, dtype=np.float32)\n",
        "    return np.mean(vecs, axis=0).astype(np.float32)\n",
        "\n",
        "def concat_first_k_vectors(tokens, kv, k=10, vector_size=VECTOR_SIZE):\n",
        "    \"\"\"\n",
        "    Concatenate first k token vectors. If token missing -> zero vector.\n",
        "    If fewer than k tokens -> pad with zero vectors.\n",
        "    Returns numpy array length k * vector_size.\n",
        "    \"\"\"\n",
        "    vecs = []\n",
        "    for i in range(k):\n",
        "        if i < len(tokens):\n",
        "            t = tokens[i]\n",
        "            if kv is not None and t in kv.key_to_index:\n",
        "                vecs.append(kv[t])\n",
        "            else:\n",
        "                vecs.append(np.zeros(vector_size, dtype=np.float32))\n",
        "        else:\n",
        "            vecs.append(np.zeros(vector_size, dtype=np.float32))\n",
        "    return np.concatenate(vecs).astype(np.float32)"
      ],
      "metadata": {
        "id": "gF0ldHw3_36i"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, features):\n",
        "        \"\"\"\n",
        "        reviews: list of strings (optional, kept for debug)\n",
        "        labels: numpy array or list of ints\n",
        "        features: numpy array shape (N, D)\n",
        "        \"\"\"\n",
        "        assert len(labels) == len(features)\n",
        "        self.reviews = reviews\n",
        "        self.X = features.astype(np.float32)\n",
        "        self.y = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "5AEBi2X0_6ph"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden1=50, hidden2=10, n_classes=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden2, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "eYCg9cgIAGo_"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "def eval_model(model, dataloader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            out = model(X_batch)\n",
        "            pred = torch.argmax(out, dim=1).cpu().numpy()\n",
        "            preds.extend(pred.tolist())\n",
        "            trues.extend(y_batch.numpy().tolist())\n",
        "    return np.array(preds), np.array(trues)\n",
        "\n",
        "def run_training(X_train, y_train, X_val, y_val, n_classes=2,\n",
        "                 hidden1=50, hidden2=10, lr=1e-3, weight_decay=1e-5,\n",
        "                 batch_size=256, epochs=10, patience=None):\n",
        "    # scale features (fit on train)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    train_ds = ReviewsDataset(None, y_train, X_train_scaled)\n",
        "    val_ds   = ReviewsDataset(None, y_val, X_val_scaled)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = MLP(input_dim=X_train.shape[1], hidden1=hidden1, hidden2=hidden2, n_classes=n_classes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        preds_val, trues_val = eval_model(model, val_loader)\n",
        "        val_acc = accuracy_score(trues_val, preds_val)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} - train_loss: {train_loss:.4f} - val_acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    # restore best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, scaler"
      ],
      "metadata": {
        "id": "gnkQjD8rAJjb"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_features_for_df(df, embed_source='pretrained', mode='avg', k=10, kv_pretrained=None, kv_own=None):\n",
        "    \"\"\"\n",
        "    embed_source: 'pretrained' | 'own'\n",
        "    mode: 'avg' | 'concat'\n",
        "    k: number of tokens to concat for 'concat' mode\n",
        "    \"\"\"\n",
        "    if embed_source == 'pretrained':\n",
        "        kv = kv_pretrained\n",
        "    elif embed_source == 'own':\n",
        "        kv = kv_own.wv if hasattr(kv_own, 'wv') else kv_own\n",
        "    else:\n",
        "        kv = None\n",
        "\n",
        "    reviews = df['review_body'].astype(str).tolist()\n",
        "    tokens_list = [tokenize(r) for r in reviews]\n",
        "\n",
        "    features = []\n",
        "    for tokens in tqdm(tokens_list, desc=f\"Building features mode={mode} src={embed_source}\"):\n",
        "        if mode == 'avg':\n",
        "            vec = avg_w2v_vector(tokens, kv, vector_size=VECTOR_SIZE)\n",
        "        elif mode == 'concat':\n",
        "            vec = concat_first_k_vectors(tokens, kv, k=k, vector_size=VECTOR_SIZE)\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'avg' or 'concat'\")\n",
        "        features.append(vec)\n",
        "    features = np.vstack(features)\n",
        "    return features"
      ],
      "metadata": {
        "id": "T0zgGIxSAOcM"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "def run_experiments(embed_source='pretrained', mode='avg', k=10,\n",
        "                    hidden1=50, hidden2=10, epochs=12, batch_size=256):\n",
        "    \"\"\"\n",
        "    embed_source: 'pretrained' or 'own'\n",
        "    mode: 'avg' or 'concat'\n",
        "    \"\"\"\n",
        "    # Prepare dataframes for binary and ternary\n",
        "    # Binary: sentiments 1 (positive label=1) and 2 (negative label=0) -- NOTE: original mapping uses 1=positive,2=negative\n",
        "    # We'll map them to 0/1 for PyTorch labels.\n",
        "    # Ternary: 1,2,3 -> map to 0,1,2\n",
        "\n",
        "    # --- Binary dataset (keep only sentiment==1 or 2)\n",
        "    train_bin = train_df[train_df['sentiment'].isin([1,2])].reset_index(drop=True)\n",
        "    test_bin  = test_df[test_df['sentiment'].isin([1,2])].reset_index(drop=True)\n",
        "    print(\"Binary sizes -> train:\", len(train_bin), \"test:\", len(test_bin))\n",
        "\n",
        "    # Build features\n",
        "    feat_train_bin = build_features_for_df(train_bin, embed_source=embed_source, mode=mode, k=k,\n",
        "                                          kv_pretrained=w2v_pretrained, kv_own=w2v_own)\n",
        "    feat_test_bin  = build_features_for_df(test_bin,  embed_source=embed_source, mode=mode, k=k,\n",
        "                                          kv_pretrained=w2v_pretrained, kv_own=w2v_own)\n",
        "\n",
        "    # labels: map 1->1, 2->0 (or you can map 1->0,2->1 depending on preference)\n",
        "    y_train_bin = (train_bin['sentiment'].astype(int).values == 1).astype(int)   # 1 -> True -> 1\n",
        "    y_test_bin  = (test_bin['sentiment'].astype(int).values == 1).astype(int)\n",
        "\n",
        "    # Train/val split inside train set (small val for checkpointing)\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_tr_bin, X_val_bin, y_tr_bin, y_val_bin = train_test_split(\n",
        "        feat_train_bin, y_train_bin, test_size=0.1, random_state=SEED, stratify=y_train_bin\n",
        "    )\n",
        "\n",
        "    model_bin, scaler_bin = run_training(\n",
        "        X_tr_bin, y_tr_bin, X_val_bin, y_val_bin,\n",
        "        n_classes=2, hidden1=hidden1, hidden2=hidden2,\n",
        "        lr=1e-3, weight_decay=1e-5, batch_size=batch_size, epochs=epochs\n",
        "    )\n",
        "\n",
        "    # Evaluate on test\n",
        "    X_test_bin_scaled = scaler_bin.transform(feat_test_bin)\n",
        "    test_ds = ReviewsDataset(None, y_test_bin, X_test_bin_scaled)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    preds_bin, trues_bin = eval_model(model_bin, test_loader)\n",
        "    acc_bin = accuracy_score(trues_bin, preds_bin)\n",
        "    print(\"\\n=== BINARY RESULTS ===\")\n",
        "    print(\"Test Accuracy:\", acc_bin)\n",
        "    print(\"Classification report:\\n\", classification_report(trues_bin, preds_bin, digits=4))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(trues_bin, preds_bin))\n",
        "\n",
        "    # --- Ternary dataset\n",
        "    train_ter = train_df[train_df['sentiment'].isin([1,2,3])].reset_index(drop=True)\n",
        "    test_ter  = test_df[test_df['sentiment'].isin([1,2,3])].reset_index(drop=True)\n",
        "    print(\"\\nTernary sizes -> train:\", len(train_ter), \"test:\", len(test_ter))\n",
        "\n",
        "    feat_train_ter = build_features_for_df(train_ter, embed_source=embed_source, mode=mode, k=k,\n",
        "                                           kv_pretrained=w2v_pretrained, kv_own=w2v_own)\n",
        "    feat_test_ter  = build_features_for_df(test_ter,  embed_source=embed_source, mode=mode, k=k,\n",
        "                                           kv_pretrained=w2v_pretrained, kv_own=w2v_own)\n",
        "    # map sentiments 1,2,3 -> 0,1,2\n",
        "    y_train_ter = (train_ter['sentiment'].astype(int).values - 1).astype(int)\n",
        "    y_test_ter  = (test_ter['sentiment'].astype(int).values - 1).astype(int)\n",
        "\n",
        "    X_tr_ter, X_val_ter, y_tr_ter, y_val_ter = train_test_split(\n",
        "        feat_train_ter, y_train_ter, test_size=0.1, random_state=SEED, stratify=y_train_ter\n",
        "    )\n",
        "\n",
        "    model_ter, scaler_ter = run_training(\n",
        "        X_tr_ter, y_tr_ter, X_val_ter, y_val_ter,\n",
        "        n_classes=3, hidden1=hidden1, hidden2=hidden2,\n",
        "        lr=1e-3, weight_decay=1e-5, batch_size=batch_size, epochs=epochs\n",
        "    )\n",
        "\n",
        "    X_test_ter_scaled = scaler_ter.transform(feat_test_ter)\n",
        "    test_ds_ter = ReviewsDataset(None, y_test_ter, X_test_ter_scaled)\n",
        "    test_loader_ter = DataLoader(test_ds_ter, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    preds_ter, trues_ter = eval_model(model_ter, test_loader_ter)\n",
        "    acc_ter = accuracy_score(trues_ter, preds_ter)\n",
        "    print(\"\\n=== TERNARY RESULTS ===\")\n",
        "    print(\"Test Accuracy:\", acc_ter)\n",
        "    print(\"Classification report:\\n\", classification_report(trues_ter, preds_ter, digits=4))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(trues_ter, preds_ter))\n",
        "\n",
        "    return {\n",
        "        'binary': {'acc': acc_bin, 'preds': preds_bin, 'trues': trues_bin},\n",
        "        'ternary': {'acc': acc_ter, 'preds': preds_ter, 'trues': trues_ter},\n",
        "        'models': {'binary': model_bin, 'ternary': model_ter},\n",
        "        'scalers': {'binary': scaler_bin, 'ternary': scaler_ter}\n",
        "    }"
      ],
      "metadata": {
        "id": "A4psvjKoAQrN"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "res_own_avg = run_experiments(embed_source='own', mode='avg', k=10, hidden1=50, hidden2=10, epochs=12, batch_size=256)\n",
        "res_own_concat = run_experiments(embed_source='own', mode='concat', k=10, hidden1=50, hidden2=10, epochs=12, batch_size=256)\n",
        "\n",
        "res_pre_avg = run_experiments(embed_source='pretrained', mode='avg', k=10, hidden1=50, hidden2=10, epochs=12, batch_size=256)\n",
        "res_pre_concat = run_experiments(embed_source='pretrained', mode='concat', k=10, hidden1=50, hidden2=10, epochs=12, batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A4qfmSsFAUdO",
        "outputId": "83786acb-8b6f-4e40-9b23-e5b728658f6f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary sizes -> train: 160000 test: 40000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=avg src=own: 100%|██████████| 160000/160000 [00:23<00:00, 6698.00it/s]\n",
            "Building features mode=avg src=own: 100%|██████████| 40000/40000 [00:06<00:00, 6188.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.3321 - val_acc: 0.8814\n",
            "Epoch 2/12 - train_loss: 0.2949 - val_acc: 0.8861\n",
            "Epoch 3/12 - train_loss: 0.2868 - val_acc: 0.8887\n",
            "Epoch 4/12 - train_loss: 0.2788 - val_acc: 0.8876\n",
            "Epoch 5/12 - train_loss: 0.2740 - val_acc: 0.8891\n",
            "Epoch 6/12 - train_loss: 0.2712 - val_acc: 0.8884\n",
            "Epoch 7/12 - train_loss: 0.2675 - val_acc: 0.8889\n",
            "Epoch 8/12 - train_loss: 0.2643 - val_acc: 0.8891\n",
            "Epoch 9/12 - train_loss: 0.2610 - val_acc: 0.8911\n",
            "Epoch 10/12 - train_loss: 0.2594 - val_acc: 0.8904\n",
            "Epoch 11/12 - train_loss: 0.2580 - val_acc: 0.8897\n",
            "Epoch 12/12 - train_loss: 0.2568 - val_acc: 0.8898\n",
            "\n",
            "=== BINARY RESULTS ===\n",
            "Test Accuracy: 0.89075\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8886    0.8935    0.8911     20000\n",
            "           1     0.8930    0.8880    0.8904     20000\n",
            "\n",
            "    accuracy                         0.8908     40000\n",
            "   macro avg     0.8908    0.8907    0.8907     40000\n",
            "weighted avg     0.8908    0.8908    0.8907     40000\n",
            "\n",
            "Confusion matrix:\n",
            " [[17871  2129]\n",
            " [ 2241 17759]]\n",
            "\n",
            "Ternary sizes -> train: 200000 test: 50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=avg src=own: 100%|██████████| 200000/200000 [00:29<00:00, 6713.05it/s]\n",
            "Building features mode=avg src=own: 100%|██████████| 50000/50000 [00:08<00:00, 6056.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.7278 - val_acc: 0.7174\n",
            "Epoch 2/12 - train_loss: 0.6846 - val_acc: 0.7206\n",
            "Epoch 3/12 - train_loss: 0.6754 - val_acc: 0.7246\n",
            "Epoch 4/12 - train_loss: 0.6681 - val_acc: 0.7218\n",
            "Epoch 5/12 - train_loss: 0.6625 - val_acc: 0.7240\n",
            "Epoch 6/12 - train_loss: 0.6597 - val_acc: 0.7239\n",
            "Epoch 7/12 - train_loss: 0.6558 - val_acc: 0.7244\n",
            "Epoch 8/12 - train_loss: 0.6521 - val_acc: 0.7265\n",
            "Epoch 9/12 - train_loss: 0.6496 - val_acc: 0.7274\n",
            "Epoch 10/12 - train_loss: 0.6484 - val_acc: 0.7287\n",
            "Epoch 11/12 - train_loss: 0.6469 - val_acc: 0.7274\n",
            "Epoch 12/12 - train_loss: 0.6452 - val_acc: 0.7273\n",
            "\n",
            "=== TERNARY RESULTS ===\n",
            "Test Accuracy: 0.72692\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7579    0.8606    0.8060     20000\n",
            "           1     0.7459    0.8427    0.7914     20000\n",
            "           2     0.4857    0.2279    0.3102     10000\n",
            "\n",
            "    accuracy                         0.7269     50000\n",
            "   macro avg     0.6632    0.6437    0.6359     50000\n",
            "weighted avg     0.6987    0.7269    0.7010     50000\n",
            "\n",
            "Confusion matrix:\n",
            " [[17212  1624  1164]\n",
            " [ 1896 16855  1249]\n",
            " [ 3602  4119  2279]]\n",
            "Binary sizes -> train: 160000 test: 40000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=concat src=own: 100%|██████████| 160000/160000 [00:06<00:00, 23225.45it/s]\n",
            "Building features mode=concat src=own: 100%|██████████| 40000/40000 [00:01<00:00, 39589.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.4562 - val_acc: 0.7953\n",
            "Epoch 2/12 - train_loss: 0.4115 - val_acc: 0.7975\n",
            "Epoch 3/12 - train_loss: 0.3904 - val_acc: 0.7999\n",
            "Epoch 4/12 - train_loss: 0.3723 - val_acc: 0.8023\n",
            "Epoch 5/12 - train_loss: 0.3549 - val_acc: 0.8011\n",
            "Epoch 6/12 - train_loss: 0.3400 - val_acc: 0.7980\n",
            "Epoch 7/12 - train_loss: 0.3262 - val_acc: 0.7995\n",
            "Epoch 8/12 - train_loss: 0.3117 - val_acc: 0.7959\n",
            "Epoch 9/12 - train_loss: 0.3019 - val_acc: 0.7972\n",
            "Epoch 10/12 - train_loss: 0.2920 - val_acc: 0.8015\n",
            "Epoch 11/12 - train_loss: 0.2812 - val_acc: 0.7996\n",
            "Epoch 12/12 - train_loss: 0.2735 - val_acc: 0.7996\n",
            "\n",
            "=== BINARY RESULTS ===\n",
            "Test Accuracy: 0.79945\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8075    0.7864    0.7968     20000\n",
            "           1     0.7918    0.8125    0.8020     20000\n",
            "\n",
            "    accuracy                         0.7994     40000\n",
            "   macro avg     0.7997    0.7994    0.7994     40000\n",
            "weighted avg     0.7997    0.7994    0.7994     40000\n",
            "\n",
            "Confusion matrix:\n",
            " [[15728  4272]\n",
            " [ 3750 16250]]\n",
            "\n",
            "Ternary sizes -> train: 200000 test: 50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=concat src=own: 100%|██████████| 200000/200000 [00:06<00:00, 32080.09it/s]\n",
            "Building features mode=concat src=own: 100%|██████████| 50000/50000 [00:01<00:00, 40013.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.8518 - val_acc: 0.6444\n",
            "Epoch 2/12 - train_loss: 0.7984 - val_acc: 0.6491\n",
            "Epoch 3/12 - train_loss: 0.7776 - val_acc: 0.6494\n",
            "Epoch 4/12 - train_loss: 0.7607 - val_acc: 0.6485\n",
            "Epoch 5/12 - train_loss: 0.7456 - val_acc: 0.6491\n",
            "Epoch 6/12 - train_loss: 0.7317 - val_acc: 0.6477\n",
            "Epoch 7/12 - train_loss: 0.7196 - val_acc: 0.6488\n",
            "Epoch 8/12 - train_loss: 0.7084 - val_acc: 0.6477\n",
            "Epoch 9/12 - train_loss: 0.6987 - val_acc: 0.6466\n",
            "Epoch 10/12 - train_loss: 0.6867 - val_acc: 0.6462\n",
            "Epoch 11/12 - train_loss: 0.6770 - val_acc: 0.6444\n",
            "Epoch 12/12 - train_loss: 0.6696 - val_acc: 0.6447\n",
            "\n",
            "=== TERNARY RESULTS ===\n",
            "Test Accuracy: 0.64454\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6533    0.7990    0.7188     20000\n",
            "           1     0.6729    0.7254    0.6982     20000\n",
            "           2     0.4372    0.1740    0.2489     10000\n",
            "\n",
            "    accuracy                         0.6445     50000\n",
            "   macro avg     0.5878    0.5661    0.5553     50000\n",
            "weighted avg     0.6179    0.6445    0.6166     50000\n",
            "\n",
            "Confusion matrix:\n",
            " [[15979  2986  1035]\n",
            " [ 4287 14508  1205]\n",
            " [ 4194  4066  1740]]\n",
            "Binary sizes -> train: 160000 test: 40000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=avg src=pretrained: 100%|██████████| 160000/160000 [00:22<00:00, 7208.71it/s]\n",
            "Building features mode=avg src=pretrained: 100%|██████████| 40000/40000 [00:05<00:00, 6746.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.3990 - val_acc: 0.8453\n",
            "Epoch 2/12 - train_loss: 0.3561 - val_acc: 0.8512\n",
            "Epoch 3/12 - train_loss: 0.3457 - val_acc: 0.8562\n",
            "Epoch 4/12 - train_loss: 0.3383 - val_acc: 0.8566\n",
            "Epoch 5/12 - train_loss: 0.3331 - val_acc: 0.8610\n",
            "Epoch 6/12 - train_loss: 0.3267 - val_acc: 0.8590\n",
            "Epoch 7/12 - train_loss: 0.3228 - val_acc: 0.8618\n",
            "Epoch 8/12 - train_loss: 0.3198 - val_acc: 0.8599\n",
            "Epoch 9/12 - train_loss: 0.3171 - val_acc: 0.8638\n",
            "Epoch 10/12 - train_loss: 0.3150 - val_acc: 0.8616\n",
            "Epoch 11/12 - train_loss: 0.3111 - val_acc: 0.8636\n",
            "Epoch 12/12 - train_loss: 0.3093 - val_acc: 0.8629\n",
            "\n",
            "=== BINARY RESULTS ===\n",
            "Test Accuracy: 0.862675\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8551    0.8733    0.8641     20000\n",
            "           1     0.8706    0.8520    0.8612     20000\n",
            "\n",
            "    accuracy                         0.8627     40000\n",
            "   macro avg     0.8628    0.8627    0.8627     40000\n",
            "weighted avg     0.8628    0.8627    0.8627     40000\n",
            "\n",
            "Confusion matrix:\n",
            " [[17467  2533]\n",
            " [ 2960 17040]]\n",
            "\n",
            "Ternary sizes -> train: 200000 test: 50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=avg src=pretrained: 100%|██████████| 200000/200000 [00:28<00:00, 7047.64it/s]\n",
            "Building features mode=avg src=pretrained: 100%|██████████| 50000/50000 [00:07<00:00, 6635.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.7955 - val_acc: 0.6853\n",
            "Epoch 2/12 - train_loss: 0.7488 - val_acc: 0.6905\n",
            "Epoch 3/12 - train_loss: 0.7349 - val_acc: 0.6927\n",
            "Epoch 4/12 - train_loss: 0.7277 - val_acc: 0.6952\n",
            "Epoch 5/12 - train_loss: 0.7229 - val_acc: 0.6986\n",
            "Epoch 6/12 - train_loss: 0.7191 - val_acc: 0.6964\n",
            "Epoch 7/12 - train_loss: 0.7153 - val_acc: 0.6987\n",
            "Epoch 8/12 - train_loss: 0.7109 - val_acc: 0.6991\n",
            "Epoch 9/12 - train_loss: 0.7091 - val_acc: 0.7000\n",
            "Epoch 10/12 - train_loss: 0.7070 - val_acc: 0.6999\n",
            "Epoch 11/12 - train_loss: 0.7049 - val_acc: 0.7014\n",
            "Epoch 12/12 - train_loss: 0.7023 - val_acc: 0.6987\n",
            "\n",
            "=== TERNARY RESULTS ===\n",
            "Test Accuracy: 0.70138\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7549    0.8081    0.7806     20000\n",
            "           1     0.6859    0.8673    0.7660     20000\n",
            "           2     0.4732    0.1561    0.2348     10000\n",
            "\n",
            "    accuracy                         0.7014     50000\n",
            "   macro avg     0.6380    0.6105    0.5938     50000\n",
            "weighted avg     0.6709    0.7014    0.6656     50000\n",
            "\n",
            "Confusion matrix:\n",
            " [[16162  2849   989]\n",
            " [ 1905 17346   749]\n",
            " [ 3343  5096  1561]]\n",
            "Binary sizes -> train: 160000 test: 40000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=concat src=pretrained: 100%|██████████| 160000/160000 [00:04<00:00, 34788.67it/s]\n",
            "Building features mode=concat src=pretrained: 100%|██████████| 40000/40000 [00:01<00:00, 38890.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.4996 - val_acc: 0.7682\n",
            "Epoch 2/12 - train_loss: 0.4506 - val_acc: 0.7789\n",
            "Epoch 3/12 - train_loss: 0.4292 - val_acc: 0.7812\n",
            "Epoch 4/12 - train_loss: 0.4113 - val_acc: 0.7869\n",
            "Epoch 5/12 - train_loss: 0.3947 - val_acc: 0.7846\n",
            "Epoch 6/12 - train_loss: 0.3822 - val_acc: 0.7837\n",
            "Epoch 7/12 - train_loss: 0.3705 - val_acc: 0.7866\n",
            "Epoch 8/12 - train_loss: 0.3573 - val_acc: 0.7835\n",
            "Epoch 9/12 - train_loss: 0.3446 - val_acc: 0.7841\n",
            "Epoch 10/12 - train_loss: 0.3371 - val_acc: 0.7848\n",
            "Epoch 11/12 - train_loss: 0.3267 - val_acc: 0.7799\n",
            "Epoch 12/12 - train_loss: 0.3193 - val_acc: 0.7813\n",
            "\n",
            "=== BINARY RESULTS ===\n",
            "Test Accuracy: 0.783375\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7983    0.7583    0.7778     20000\n",
            "           1     0.7699    0.8084    0.7887     20000\n",
            "\n",
            "    accuracy                         0.7834     40000\n",
            "   macro avg     0.7841    0.7834    0.7832     40000\n",
            "weighted avg     0.7841    0.7834    0.7832     40000\n",
            "\n",
            "Confusion matrix:\n",
            " [[15167  4833]\n",
            " [ 3832 16168]]\n",
            "\n",
            "Ternary sizes -> train: 200000 test: 50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building features mode=concat src=pretrained: 100%|██████████| 200000/200000 [00:05<00:00, 35544.64it/s]\n",
            "Building features mode=concat src=pretrained: 100%|██████████| 50000/50000 [00:01<00:00, 37204.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 - train_loss: 0.8844 - val_acc: 0.6214\n",
            "Epoch 2/12 - train_loss: 0.8340 - val_acc: 0.6272\n",
            "Epoch 3/12 - train_loss: 0.8123 - val_acc: 0.6307\n",
            "Epoch 4/12 - train_loss: 0.7963 - val_acc: 0.6303\n",
            "Epoch 5/12 - train_loss: 0.7831 - val_acc: 0.6338\n",
            "Epoch 6/12 - train_loss: 0.7706 - val_acc: 0.6358\n",
            "Epoch 7/12 - train_loss: 0.7599 - val_acc: 0.6364\n",
            "Epoch 8/12 - train_loss: 0.7498 - val_acc: 0.6360\n",
            "Epoch 9/12 - train_loss: 0.7400 - val_acc: 0.6355\n",
            "Epoch 10/12 - train_loss: 0.7308 - val_acc: 0.6338\n",
            "Epoch 11/12 - train_loss: 0.7242 - val_acc: 0.6342\n",
            "Epoch 12/12 - train_loss: 0.7149 - val_acc: 0.6311\n",
            "\n",
            "=== TERNARY RESULTS ===\n",
            "Test Accuracy: 0.6328\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6621    0.7528    0.7045     20000\n",
            "           1     0.6318    0.7550    0.6879     20000\n",
            "           2     0.4416    0.1485    0.2223     10000\n",
            "\n",
            "    accuracy                         0.6328     50000\n",
            "   macro avg     0.5785    0.5521    0.5382     50000\n",
            "weighted avg     0.6059    0.6328    0.6014     50000\n",
            "\n",
            "Confusion matrix:\n",
            " [[15055  4052   893]\n",
            " [ 3915 15100   985]\n",
            " [ 3768  4747  1485]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "EJdZwsE2U3Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 50              # max review length (tokens)\n",
        "EMBED_DIM = 300           # must match your W2V vector size\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 12\n",
        "LR = 1e-3\n",
        "DROPOUT = 0.2"
      ],
      "metadata": {
        "id": "76RRCdAxU7Uh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n",
        "\n",
        "# Load your Word2Vec model (if not already in session)\n",
        "from gensim.models import Word2Vec\n",
        "w2v_own = Word2Vec.load(W2V_OWN_PATH)\n",
        "print(\"Loaded own w2v; vocab size:\", len(w2v_own.wv.key_to_index))\n",
        "kv = w2v_own.wv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ee7frj4sF2yy",
        "outputId": "82467572-5ba1-4620-ead2-2f5787550b27"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (200000, 3) Test: (50000, 3)\n",
            "Loaded own w2v; vocab size: 15017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "reserved_tokens = [\"<PAD>\"]  # index 0\n",
        "\n",
        "token_to_idx = {}\n",
        "idx = 1\n",
        "for tok in kv.key_to_index:   # iterates tokens in kv\n",
        "    token_to_idx[tok] = idx\n",
        "    idx += 1\n",
        "\n",
        "vocab_size = idx  # includes padding index 0\n",
        "print(\"Vocab size (with PAD=0):\", vocab_size)\n",
        "\n",
        "# Build embedding matrix: shape (vocab_size, EMBED_DIM)\n",
        "# index 0 = zeros\n",
        "embedding_matrix = np.zeros((vocab_size, EMBED_DIM), dtype=np.float32)\n",
        "for tok, i in token_to_idx.items():\n",
        "    embedding_matrix[i] = kv[tok]  # copy vector\n",
        "\n",
        "# Optionally convert to torch tensor later\n",
        "embedding_matrix = torch.tensor(embedding_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lNy0VpaZFsLN",
        "outputId": "5d521c46-fca5-4c30-d4bc-7f6adfdd537f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size (with PAD=0): 15018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_to_idx_sequence(text, token_to_idx, max_len=MAX_LEN):\n",
        "    tokens = tokenize(text)\n",
        "    indices = []\n",
        "    for t in tokens[:max_len]:\n",
        "        indices.append(token_to_idx.get(t, 0))  # 0 for PAD/OOV\n",
        "    # pad if shorter\n",
        "    if len(indices) < max_len:\n",
        "        indices.extend([0] * (max_len - len(indices)))\n",
        "    return indices\n",
        "\n",
        "# Quick test\n",
        "print(text_to_idx_sequence(\"This is a short test review. Not bad!\", token_to_idx)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W91eHpgLFyli",
        "outputId": "bc434f11-3b3c-4793-bc56-fb49b4aa7cd2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 5, 489, 687, 304, 11, 208, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SeqReviewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, token_to_idx, max_len=MAX_LEN):\n",
        "        self.texts = texts\n",
        "        self.labels = np.array(labels, dtype=np.int64)\n",
        "        self.token_to_idx = token_to_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t = self.texts[idx]\n",
        "        seq = text_to_idx_sequence(t, self.token_to_idx, self.max_len)\n",
        "        return torch.tensor(seq, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
      ],
      "metadata": {
        "id": "traBRkIiGLA1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, emb_matrix=None, conv_channels=(50,10), kernel_size=3, dropout=0.2, n_classes=2, freeze_embeddings=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        if emb_matrix is not None:\n",
        "            self.embedding.weight.data.copy_(emb_matrix)\n",
        "        if freeze_embeddings:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "\n",
        "        c1, c2 = conv_channels\n",
        "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=c1, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(in_channels=c1, out_channels=c2, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "\n",
        "        # after conv2 we do global max pooling over sequence length -> produces c2 features\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(c2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len) long\n",
        "        emb = self.embedding(x)                # (batch, seq_len, embed_dim)\n",
        "        emb = emb.permute(0, 2, 1)             # (batch, embed_dim, seq_len) for Conv1d\n",
        "        h = self.conv1(emb)                    # (batch, c1, seq_len)\n",
        "        h = self.relu(h)\n",
        "        h = self.conv2(h)                      # (batch, c2, seq_len)\n",
        "        h = self.relu(h)\n",
        "        # global max pooling over seq_len\n",
        "        h, _ = torch.max(h, dim=2)             # (batch, c2)\n",
        "        h = self.dropout(h)\n",
        "        logits = self.fc(h)                    # (batch, n_classes)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "xBgJXBNHGM-y"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_epoch_cnn(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_cnn(model, loader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            out = model(X_batch)\n",
        "            pred = torch.argmax(out, dim=1).cpu().numpy()\n",
        "            preds.extend(pred.tolist())\n",
        "            trues.extend(y_batch.numpy().tolist())\n",
        "    return np.array(preds), np.array(trues)"
      ],
      "metadata": {
        "id": "UkrUJYcOGO_3"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_cnn_experiment(train_df, test_df, token_to_idx, emb_matrix, max_len=MAX_LEN, conv_channels=(50,10),\n",
        "                       kernel_size=3, freeze_embeddings=True, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                       lr=LR, dropout=DROPOUT):\n",
        "    results = {}\n",
        "\n",
        "    # --- Binary: sentiments 1 & 2\n",
        "    train_bin = train_df[train_df['sentiment'].isin([1,2])].reset_index(drop=True)\n",
        "    test_bin  = test_df[test_df['sentiment'].isin([1,2])].reset_index(drop=True)\n",
        "    print(\"Binary sizes ->\", len(train_bin), len(test_bin))\n",
        "    # map labels: 1 -> 1, 2 -> 0  (you can also map vice-versa)\n",
        "    y_train_bin = (train_bin['sentiment'].astype(int).values == 1).astype(int)\n",
        "    y_test_bin  = (test_bin['sentiment'].astype(int).values == 1).astype(int)\n",
        "\n",
        "    train_ds_bin = SeqReviewsDataset(train_bin['review_body'].astype(str).tolist(), y_train_bin, token_to_idx, max_len)\n",
        "    test_ds_bin  = SeqReviewsDataset(test_bin['review_body'].astype(str).tolist(), y_test_bin, token_to_idx, max_len)\n",
        "\n",
        "    tr_loader = DataLoader(train_ds_bin, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    te_loader = DataLoader(test_ds_bin, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model_bin = SimpleCNN(vocab_size=emb_matrix.shape[0], embed_dim=emb_matrix.shape[1],\n",
        "                          emb_matrix=emb_matrix, conv_channels=conv_channels, kernel_size=kernel_size,\n",
        "                          dropout=dropout, n_classes=2, freeze_embeddings=freeze_embeddings).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_bin.parameters()), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_state = None\n",
        "    for epoch in range(1, epochs+1):\n",
        "        tr_loss = train_epoch_cnn(model_bin, tr_loader, optimizer, criterion)\n",
        "        preds_val, trues_val = eval_cnn(model_bin, te_loader)\n",
        "        acc = accuracy_score(trues_val, preds_val)\n",
        "        print(f\"Epoch {epoch}/{epochs} - train_loss: {tr_loss:.4f} - test_acc: {acc:.4f}\")\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_state = model_bin.state_dict()\n",
        "\n",
        "    if best_state is not None:\n",
        "        model_bin.load_state_dict(best_state)\n",
        "\n",
        "    preds_bin, trues_bin = eval_cnn(model_bin, te_loader)\n",
        "    acc_bin = accuracy_score(trues_bin, preds_bin)\n",
        "    print(\"\\n=== BINARY TEST ACC:\", acc_bin)\n",
        "    print(classification_report(trues_bin, preds_bin, digits=4))\n",
        "    print(\"Confusion:\\n\", confusion_matrix(trues_bin, preds_bin))\n",
        "    results['binary'] = {'acc': acc_bin, 'preds': preds_bin, 'trues': trues_bin}\n",
        "\n",
        "    # --- Ternary: sentiments 1,2,3\n",
        "    train_ter = train_df[train_df['sentiment'].isin([1,2,3])].reset_index(drop=True)\n",
        "    test_ter  = test_df[test_df['sentiment'].isin([1,2,3])].reset_index(drop=True)\n",
        "    print(\"\\nTernary sizes ->\", len(train_ter), len(test_ter))\n",
        "\n",
        "    # labels -> 0,1,2\n",
        "    y_train_ter = (train_ter['sentiment'].astype(int).values - 1).astype(int)\n",
        "    y_test_ter  = (test_ter['sentiment'].astype(int).values - 1).astype(int)\n",
        "\n",
        "    train_ds_ter = SeqReviewsDataset(train_ter['review_body'].astype(str).tolist(), y_train_ter, token_to_idx, max_len)\n",
        "    test_ds_ter  = SeqReviewsDataset(test_ter['review_body'].astype(str).tolist(), y_test_ter, token_to_idx, max_len)\n",
        "\n",
        "    tr_loader_ter = DataLoader(train_ds_ter, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    te_loader_ter = DataLoader(test_ds_ter, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model_ter = SimpleCNN(vocab_size=emb_matrix.shape[0], embed_dim=emb_matrix.shape[1],\n",
        "                          emb_matrix=emb_matrix, conv_channels=conv_channels, kernel_size=kernel_size,\n",
        "                          dropout=dropout, n_classes=3, freeze_embeddings=freeze_embeddings).to(device)\n",
        "\n",
        "    optimizer2 = torch.optim.Adam(filter(lambda p: p.requires_grad, model_ter.parameters()), lr=lr)\n",
        "    criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc2 = 0.0\n",
        "    best_state2 = None\n",
        "    for epoch in range(1, epochs+1):\n",
        "        tr_loss = train_epoch_cnn(model_ter, tr_loader_ter, optimizer2, criterion2)\n",
        "        preds_val, trues_val = eval_cnn(model_ter, te_loader_ter)\n",
        "        acc = accuracy_score(trues_val, preds_val)\n",
        "        print(f\"Epoch {epoch}/{epochs} - train_loss: {tr_loss:.4f} - test_acc: {acc:.4f}\")\n",
        "        if acc > best_acc2:\n",
        "            best_acc2 = acc\n",
        "            best_state2 = model_ter.state_dict()\n",
        "\n",
        "    if best_state2 is not None:\n",
        "        model_ter.load_state_dict(best_state2)\n",
        "\n",
        "    preds_ter, trues_ter = eval_cnn(model_ter, te_loader_ter)\n",
        "    acc_ter = accuracy_score(trues_ter, preds_ter)\n",
        "    print(\"\\n=== TERNARY TEST ACC:\", acc_ter)\n",
        "    print(classification_report(trues_ter, preds_ter, digits=4))\n",
        "    print(\"Confusion:\\n\", confusion_matrix(trues_ter, preds_ter))\n",
        "    results['ternary'] = {'acc': acc_ter, 'preds': preds_ter, 'trues': trues_ter}\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Em4e_alqGRZ_"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert embedding_matrix to float tensor on CPU (the SimpleCNN copies it into model)\n",
        "emb_matrix = embedding_matrix.float().cpu()\n",
        "res = run_cnn_experiment(train_df, test_df, token_to_idx, emb_matrix,\n",
        "                         max_len=MAX_LEN, conv_channels=(50,10),\n",
        "                         kernel_size=3, freeze_embeddings=True,\n",
        "                         epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, dropout=DROPOUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6xorDe-KGTDX",
        "outputId": "ccebf3d9-1f6b-428a-8875-db4e189d6579"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary sizes -> 160000 40000\n",
            "Epoch 1/12 - train_loss: 0.3760 - test_acc: 0.8659\n",
            "Epoch 2/12 - train_loss: 0.3175 - test_acc: 0.8770\n",
            "Epoch 3/12 - train_loss: 0.3017 - test_acc: 0.8834\n",
            "Epoch 4/12 - train_loss: 0.2894 - test_acc: 0.8840\n",
            "Epoch 5/12 - train_loss: 0.2767 - test_acc: 0.8864\n",
            "Epoch 6/12 - train_loss: 0.2685 - test_acc: 0.8878\n",
            "Epoch 7/12 - train_loss: 0.2601 - test_acc: 0.8893\n",
            "Epoch 8/12 - train_loss: 0.2514 - test_acc: 0.8890\n",
            "Epoch 9/12 - train_loss: 0.2438 - test_acc: 0.8872\n",
            "Epoch 10/12 - train_loss: 0.2367 - test_acc: 0.8887\n",
            "Epoch 11/12 - train_loss: 0.2290 - test_acc: 0.8908\n",
            "Epoch 12/12 - train_loss: 0.2230 - test_acc: 0.8882\n",
            "\n",
            "=== BINARY TEST ACC: 0.888175\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8767    0.9034    0.8899     20000\n",
            "           1     0.9004    0.8729    0.8864     20000\n",
            "\n",
            "    accuracy                         0.8882     40000\n",
            "   macro avg     0.8885    0.8882    0.8881     40000\n",
            "weighted avg     0.8885    0.8882    0.8881     40000\n",
            "\n",
            "Confusion:\n",
            " [[18069  1931]\n",
            " [ 2542 17458]]\n",
            "\n",
            "Ternary sizes -> 200000 50000\n",
            "Epoch 1/12 - train_loss: 0.7658 - test_acc: 0.7066\n",
            "Epoch 2/12 - train_loss: 0.7072 - test_acc: 0.7181\n",
            "Epoch 3/12 - train_loss: 0.6904 - test_acc: 0.7217\n",
            "Epoch 4/12 - train_loss: 0.6768 - test_acc: 0.7212\n",
            "Epoch 5/12 - train_loss: 0.6654 - test_acc: 0.7239\n",
            "Epoch 6/12 - train_loss: 0.6567 - test_acc: 0.7263\n",
            "Epoch 7/12 - train_loss: 0.6487 - test_acc: 0.7282\n",
            "Epoch 8/12 - train_loss: 0.6416 - test_acc: 0.7270\n",
            "Epoch 9/12 - train_loss: 0.6336 - test_acc: 0.7246\n",
            "Epoch 10/12 - train_loss: 0.6268 - test_acc: 0.7278\n",
            "Epoch 11/12 - train_loss: 0.6201 - test_acc: 0.7272\n",
            "Epoch 12/12 - train_loss: 0.6152 - test_acc: 0.7282\n",
            "\n",
            "=== TERNARY TEST ACC: 0.72818\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7750    0.8486    0.8101     20000\n",
            "           1     0.7395    0.8430    0.7879     20000\n",
            "           2     0.4860    0.2577    0.3368     10000\n",
            "\n",
            "    accuracy                         0.7282     50000\n",
            "   macro avg     0.6669    0.6498    0.6449     50000\n",
            "weighted avg     0.7030    0.7282    0.7066     50000\n",
            "\n",
            "Confusion:\n",
            " [[16971  1743  1286]\n",
            " [ 1700 16861  1439]\n",
            " [ 3227  4196  2577]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== PRETRAINED CNN VERSION =====\n",
        "\n",
        "import gensim.downloader as api\n",
        "from collections import Counter\n",
        "\n",
        "print(\"Loading pretrained GoogleNews vectors...\")\n",
        "w2v_pretrained = api.load(\"word2vec-google-news-300\")\n",
        "kv_pre = w2v_pretrained\n",
        "print(\"Loaded pretrained vectors.\")\n",
        "\n",
        "# Build dataset-specific vocabulary (only words appearing in train+test)\n",
        "print(\"Building compact vocabulary from dataset...\")\n",
        "all_texts = pd.concat([\n",
        "    train_df['review_body'].astype(str),\n",
        "    test_df['review_body'].astype(str)\n",
        "]).tolist()\n",
        "\n",
        "token_set = set()\n",
        "for text in all_texts:\n",
        "    token_set.update(tokenize(text))\n",
        "\n",
        "print(\"Unique dataset tokens:\", len(token_set))\n",
        "\n",
        "# Create token -> index mapping (0 reserved for PAD)\n",
        "token_to_idx_pre = {}\n",
        "idx = 1\n",
        "for tok in token_set:\n",
        "    token_to_idx_pre[tok] = idx\n",
        "    idx += 1\n",
        "\n",
        "vocab_size_pre = idx\n",
        "print(\"Compact vocab size:\", vocab_size_pre)\n",
        "\n",
        "# Build compact embedding matrix\n",
        "EMBED_DIM = kv_pre.vector_size\n",
        "embedding_matrix_pre = np.zeros((vocab_size_pre, EMBED_DIM), dtype=np.float32)\n",
        "\n",
        "missing = 0\n",
        "for tok, i in token_to_idx_pre.items():\n",
        "    if tok in kv_pre.key_to_index:\n",
        "        embedding_matrix_pre[i] = kv_pre[tok]\n",
        "    else:\n",
        "        missing += 1\n",
        "\n",
        "print(\"Missing tokens from pretrained:\", missing)\n",
        "\n",
        "# Convert to torch tensor\n",
        "emb_matrix_pre = torch.tensor(embedding_matrix_pre).float().cpu()\n",
        "\n",
        "# Run CNN experiment (same structure as your own model)\n",
        "res_pre = run_cnn_experiment(\n",
        "    train_df,\n",
        "    test_df,\n",
        "    token_to_idx_pre,\n",
        "    emb_matrix_pre,\n",
        "    max_len=MAX_LEN,\n",
        "    conv_channels=(50,10),\n",
        "    kernel_size=3,\n",
        "    freeze_embeddings=True,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=LR,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "print(\"\\n===== PRETRAINED CNN RESULTS =====\")\n",
        "print(\"Binary Accuracy:\", res_pre['binary']['acc'])\n",
        "print(\"Ternary Accuracy:\", res_pre['ternary']['acc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iAQwiICMGV93",
        "outputId": "c6dcab6c-654d-49d5-c2a5-91235c342dd8"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained GoogleNews vectors...\n",
            "Loaded pretrained vectors.\n",
            "Building compact vocabulary from dataset...\n",
            "Unique dataset tokens: 70895\n",
            "Compact vocab size: 70896\n",
            "Missing tokens from pretrained: 26605\n",
            "Binary sizes -> 160000 40000\n",
            "Epoch 1/12 - train_loss: 0.4394 - test_acc: 0.8423\n",
            "Epoch 2/12 - train_loss: 0.3523 - test_acc: 0.8629\n",
            "Epoch 3/12 - train_loss: 0.3299 - test_acc: 0.8696\n",
            "Epoch 4/12 - train_loss: 0.3142 - test_acc: 0.8710\n",
            "Epoch 5/12 - train_loss: 0.3028 - test_acc: 0.8730\n",
            "Epoch 6/12 - train_loss: 0.2922 - test_acc: 0.8748\n",
            "Epoch 7/12 - train_loss: 0.2827 - test_acc: 0.8763\n",
            "Epoch 8/12 - train_loss: 0.2752 - test_acc: 0.8780\n",
            "Epoch 9/12 - train_loss: 0.2683 - test_acc: 0.8761\n",
            "Epoch 10/12 - train_loss: 0.2606 - test_acc: 0.8772\n",
            "Epoch 11/12 - train_loss: 0.2544 - test_acc: 0.8783\n",
            "Epoch 12/12 - train_loss: 0.2478 - test_acc: 0.8761\n",
            "\n",
            "=== BINARY TEST ACC: 0.876125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8801    0.8710    0.8755     20000\n",
            "           1     0.8723    0.8813    0.8768     20000\n",
            "\n",
            "    accuracy                         0.8761     40000\n",
            "   macro avg     0.8762    0.8761    0.8761     40000\n",
            "weighted avg     0.8762    0.8761    0.8761     40000\n",
            "\n",
            "Confusion:\n",
            " [[17419  2581]\n",
            " [ 2374 17626]]\n",
            "\n",
            "Ternary sizes -> 200000 50000\n",
            "Epoch 1/12 - train_loss: 0.8235 - test_acc: 0.6846\n",
            "Epoch 2/12 - train_loss: 0.7359 - test_acc: 0.7038\n",
            "Epoch 3/12 - train_loss: 0.7119 - test_acc: 0.7108\n",
            "Epoch 4/12 - train_loss: 0.6975 - test_acc: 0.7111\n",
            "Epoch 5/12 - train_loss: 0.6856 - test_acc: 0.7164\n",
            "Epoch 6/12 - train_loss: 0.6749 - test_acc: 0.7182\n",
            "Epoch 7/12 - train_loss: 0.6674 - test_acc: 0.7194\n",
            "Epoch 8/12 - train_loss: 0.6608 - test_acc: 0.7173\n",
            "Epoch 9/12 - train_loss: 0.6525 - test_acc: 0.7206\n",
            "Epoch 10/12 - train_loss: 0.6473 - test_acc: 0.7204\n",
            "Epoch 11/12 - train_loss: 0.6420 - test_acc: 0.7205\n",
            "Epoch 12/12 - train_loss: 0.6380 - test_acc: 0.7204\n",
            "\n",
            "=== TERNARY TEST ACC: 0.72042\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7794    0.8338    0.8057     20000\n",
            "           1     0.7020    0.8787    0.7804     20000\n",
            "           2     0.4962    0.1772    0.2611     10000\n",
            "\n",
            "    accuracy                         0.7204     50000\n",
            "   macro avg     0.6592    0.6299    0.6158     50000\n",
            "weighted avg     0.6918    0.7204    0.6867     50000\n",
            "\n",
            "Confusion:\n",
            " [[16676  2371   953]\n",
            " [ 1581 17573   846]\n",
            " [ 3139  5089  1772]]\n",
            "\n",
            "===== PRETRAINED CNN RESULTS =====\n",
            "Binary Accuracy: 0.876125\n",
            "Ternary Accuracy: 0.72042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CragofXhPP-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}